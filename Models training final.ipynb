{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141869c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from joblib import dump\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, confusion_matrix, roc_curve, auc, \\\n",
    "    roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "SCORING = {'accuracy': 'accuracy', 'precision': make_scorer(precision_score), 'recall': make_scorer(recall_score),\n",
    "           'f1': make_scorer(f1_score),\n",
    "           'AUC': make_scorer(roc_auc_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52656c",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23299339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data and creating test/train features\n",
    "dfApache = pd.read_excel('ApacheSample.xlsx')\n",
    "dfJunit = pd.read_excel('junitSample.xlsx')\n",
    "dfOkhttp =  pd.read_excel('okhttpSample.xlsx')\n",
    "dfRetrofit = pd.read_excel('retrofitSample.xlsx')\n",
    "dfSpringBoot = pd.read_excel('springBootSample.xlsx')\n",
    "# Specify the column name for the labels\n",
    "label_column = 'label'\n",
    "message_column = 'new_message1'\n",
    "\n",
    "def mylog():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logfile = \"./log/\" + str(datetime.datetime.now().month) + \"-\" + str(datetime.datetime.now().day) + \"-\" + str(\n",
    "        datetime.datetime.now().hour) + \"-\" + str(datetime.datetime.now().minute) + \\\n",
    "              os.path.split(__file__)[-1].split(\".\")[0] + '.log'\n",
    "    fileHandler = logging.FileHandler(logfile, mode='w', encoding='UTF-8')\n",
    "    fileHandler.setLevel(logging.NOTSET)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    logger.addHandler(fileHandler)\n",
    "    return logger\n",
    "\n",
    "# Define the preprocess_text function to perform text preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_text)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Preprocess the commit message text\n",
    "dfApache['new_message1'] = dfApache['new_message1'].apply(preprocess_text)\n",
    "dfJunit['new_message1'] = dfJunit['new_message1'].apply(preprocess_text)\n",
    "dfOkhttp['new_message1'] = dfOkhttp['new_message1'].apply(preprocess_text)\n",
    "dfRetrofit['new_message1'] = dfRetrofit['new_message1'].apply(preprocess_text)\n",
    "dfSpringBoot['new_message1'] = dfSpringBoot['new_message1'].apply(preprocess_text)\n",
    "\n",
    "#splitting the data\n",
    "apacheTrain, apacheTest = train_test_split(dfApache, test_size=0.15, random_state=42)\n",
    "junitTrain, junitTest = train_test_split(dfJunit, test_size=0.15, random_state=42)\n",
    "okhttpTrain, okhttpTest= train_test_split(dfOkhttp, test_size=0.15, random_state=42)\n",
    "retrofitTrain, retrofitTest = train_test_split(dfRetrofit, test_size=0.15, random_state=42)\n",
    "springBootTrain, springBootTest = train_test_split(dfSpringBoot, test_size=0.15, random_state=42)\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_trainApache = vectorizer.fit_transform(apacheTrain['new_message1'])\n",
    "X_testApache = vectorizer.transform(apacheTest['new_message1'])\n",
    "\n",
    "X_trainJunit = vectorizer.fit_transform(junitTrain['new_message1'])\n",
    "X_testJunit = vectorizer.transform(junitTest['new_message1'])\n",
    "\n",
    "X_trainOkhttp = vectorizer.fit_transform(okhttpTrain['new_message1'])\n",
    "X_testOkhttp = vectorizer.transform(okhttpTest['new_message1'])\n",
    "\n",
    "X_trainRetrofit = vectorizer.fit_transform(retrofitTrain['new_message1'])\n",
    "X_testRetrofit = vectorizer.transform(retrofitTest['new_message1'])\n",
    "\n",
    "X_trainSpringboot = vectorizer.fit_transform(springBootTrain['new_message1'])\n",
    "X_testSpringboot = vectorizer.transform(springBootTest['new_message1'])\n",
    "\n",
    "# Get the corresponding labels for the training and test sets\n",
    "y_trainApache = apacheTrain[label_column]\n",
    "y_testApache = apacheTest[label_column]\n",
    "\n",
    "y_trainJunit = junitTrain[label_column]\n",
    "y_testJunit = junitTest[label_column]\n",
    "\n",
    "y_trainOkhttp = okhttpTrain[label_column]\n",
    "y_testOkhttp = okhttpTest[label_column]\n",
    "\n",
    "y_trainRetrofit = retrofitTrain[label_column]\n",
    "y_testRetrofit = retrofitTest[label_column]\n",
    "\n",
    "y_trainSpringboot = springBootTrain[label_column]\n",
    "y_testSpringboot = springBootTest[label_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654c6f2",
   "metadata": {},
   "source": [
    "## Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2783ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def get_score_by_grid(grid: GridSearchCV):\n",
    "    print(\"GridSearchCV is complete!\")\n",
    "    accuRank = grid.cv_results_['rank_test_accuracy']\n",
    "    preMean = grid.cv_results_['mean_test_precision']\n",
    "    bestParam = grid.cv_results_['params']\n",
    "    bestIndex = grid.best_index_\n",
    "    i = bestIndex\n",
    "    rank = 1\n",
    "\n",
    "    while preMean[i] < 0.5:\n",
    "        rank += 1\n",
    "        indx = 0\n",
    "        if rank > 20:\n",
    "            break\n",
    "        for num in accuRank:\n",
    "            if num == rank:\n",
    "                i = indx\n",
    "                break\n",
    "            indx += 1\n",
    "    bestIndex = i\n",
    "\n",
    "    res = \"refit by:\" + str(grid.refit) + \" Parameters: \" + str(bestParam[bestIndex])\n",
    "    return bestParam[bestIndex]\n",
    "\n",
    "\n",
    "def LRClassifier(trainFeatures, trainLabels):\n",
    "    parameters = {'C': np.linspace(0.0001, 20, 20),\n",
    "                  'random_state': np.arange(1, 5),\n",
    "                  'solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\"],\n",
    "                  'multi_class': ['ovr'],\n",
    "                  'dual': [False],\n",
    "                  'verbose': [False],\n",
    "                  'max_iter': [500]\n",
    "                  }\n",
    "    fold = KFold(n_splits=10, random_state=5, shuffle=True)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        grid = GridSearchCV(LogisticRegression(), parameters, scoring=SCORING, refit=\"accuracy\", cv=fold, n_jobs=25)\n",
    "        grid.fit(trainFeatures, trainLabels)\n",
    "    \n",
    "    bestParameter = get_score_by_grid(grid)\n",
    "    print(\"LR Best: using %s \" % (bestParameter))\n",
    "    model = LogisticRegression(C=bestParameter['C'], random_state=bestParameter['random_state'],\n",
    "                               solver=bestParameter['solver'], multi_class='ovr', dual=False, verbose=False,\n",
    "                               max_iter=500)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ace04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### do not run for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71361bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Generating model results for each project\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    LRModelApache = LRClassifier(X_trainApache, y_trainApache)\n",
    "    LRModelApache.fit(X_trainApache, y_trainApache)\n",
    "\n",
    "    LRModelJunit = LRClassifier(X_trainJunit, y_trainJunit)\n",
    "    LRModelJunit.fit(X_trainJunit, y_trainJunit)\n",
    "\n",
    "    LRModelOkhttp = LRClassifier(X_trainOkhttp, y_trainOkhttp)\n",
    "    LRModelOkhttp.fit(X_trainOkhttp, y_trainOkhttp)\n",
    "\n",
    "    LRModelRetrofit = LRClassifier(X_trainRetrofit, y_trainRetrofit)\n",
    "    LRModelRetrofit.fit(X_trainRetrofit, y_trainRetrofit)\n",
    "\n",
    "    LRModelSpringboot = LRClassifier(X_trainSpringboot, y_trainSpringboot)\n",
    "    LRModelSpringboot.fit(X_trainSpringboot, y_trainSpringboot)\n",
    "\n",
    "# Generating predicts and model accuracy\n",
    "predictionsApache = LRModelApache.predict(X_testApache)\n",
    "accuracyApache = accuracy_score(y_testApache, predictionsApache)\n",
    "\n",
    "predictionsJunit = LRModelJunit.predict(X_testJunit)\n",
    "accuracyJunit = accuracy_score(y_testJunit, predictionsJunit)\n",
    "\n",
    "predictionsOkhttp = LRModelOkhttp.predict(X_testOkhttp)\n",
    "accuracyOkhttp = accuracy_score(y_testOkhttp, predictionsOkhttp)\n",
    "\n",
    "predictionsRetrofit = LRModelRetrofit.predict(X_testRetrofit)\n",
    "accuracyRetrofit = accuracy_score(y_testRetrofit, predictionsRetrofit)\n",
    "\n",
    "predictionsSpringboot = LRModelSpringboot.predict(X_testSpringboot)\n",
    "accuracySpringboot = accuracy_score(y_testSpringboot, predictionsSpringboot)\n",
    "\n",
    "# Filter the test labels and predictions for c-why and c-what categories (label = 0)\n",
    "test_labelsApache = apacheTest[label_column]\n",
    "c_why_what_labelsApache = test_labelsApache[test_labelsApache == 0]\n",
    "c_why_what_predictionsApache = predictionsApache[test_labelsApache == 0]\n",
    "\n",
    "test_labelsJunit = junitTest[label_column]\n",
    "c_why_what_labelsJunit = test_labelsJunit[test_labelsJunit == 0]\n",
    "c_why_what_predictionsJunit = predictionsJunit[test_labelsJunit == 0]\n",
    "\n",
    "test_labelsOkhttp = okhttpTest[label_column]\n",
    "c_why_what_labelsOkhttp = test_labelsOkhttp[test_labelsOkhttp == 0]\n",
    "c_why_what_predictionsOkhttp = predictionsOkhttp[test_labelsOkhttp == 0]\n",
    "\n",
    "test_labelsRetrofit = retrofitTest[label_column]\n",
    "c_why_what_labelsRetrofit = test_labelsRetrofit[test_labelsRetrofit == 0]\n",
    "c_why_what_predictionsRetrofit = predictionsRetrofit[test_labelsRetrofit == 0]\n",
    "\n",
    "test_labelsSpringboot = springBootTest[label_column]\n",
    "c_why_what_labelsSpringboot = test_labelsSpringboot[test_labelsSpringboot == 0]\n",
    "c_why_what_predictionsSpringboot = predictionsSpringboot[test_labelsSpringboot == 0]\n",
    "\n",
    "# Calculate accuracy for c-why and c-what combined category\n",
    "c_why_what_accuracyApache = accuracy_score(c_why_what_labelsApache, c_why_what_predictionsApache)\n",
    "c_why_what_accuracyJunit = accuracy_score(c_why_what_labelsJunit, c_why_what_predictionsJunit)\n",
    "c_why_what_accuracyOkhttp = accuracy_score(c_why_what_labelsOkhttp, c_why_what_predictionsOkhttp)\n",
    "c_why_what_accuracyRetrofit = accuracy_score(c_why_what_labelsRetrofit, c_why_what_predictionsRetrofit)\n",
    "c_why_what_accuracySpringboot = accuracy_score(c_why_what_labelsSpringboot, c_why_what_predictionsSpringboot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63458335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy Overall for Apache Project: 0.62\n",
      "Accuracy (c-why and c-what) for Apache Project: 0.8064516129032258\n",
      "Training set shape for Apache Project: (278, 9)\n",
      "Test set shape for Apache Project: (50, 9)\n",
      "\n",
      "\n",
      "Accuracy Overall for Junit Project: 0.5102040816326531\n",
      "Accuracy (c-why and c-what) for Junit Project: 0.7857142857142857\n",
      "Training set shape for Junit Project: (273, 9)\n",
      "Test set shape for Junit Project: (49, 9)\n",
      "\n",
      "\n",
      "Accuracy Overall for Okhttp Project: 0.7254901960784313\n",
      "Accuracy (c-why and c-what) for Okhttp Project: 0.8888888888888888\n",
      "Training set shape for Okhttp Project: (287, 9)\n",
      "Test set shape for Okhttp Project: (51, 9)\n",
      "\n",
      "\n",
      "Accuracy Overall for Retrofit Project: 0.7441860465116279\n",
      "Accuracy (c-why and c-what) for Retrofit Project: 0.75\n",
      "Training set shape for Retrofit Project: (43, 9)\n",
      "Test set shape for Retrofit Project: (240, 9)\n",
      "\n",
      "\n",
      "Accuracy Overall for Springboot Project: 0.8771929824561403\n",
      "Accuracy (c-why and c-what) for Springboot Project: 0.9583333333333334\n",
      "Training set shape for Springboot Project: (57, 9)\n",
      "Test set shape for Springboot Project: (320, 9)\n"
     ]
    }
   ],
   "source": [
    "#Displaying results\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Overall for Apache Project:\", accuracyApache)\n",
    "print(\"Accuracy (c-why and c-what) for Apache Project:\", c_why_what_accuracyApache)\n",
    "print(\"Training set shape for Apache Project:\", apacheTrain.shape)\n",
    "print(\"Test set shape for Apache Project:\", apacheTest.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Overall for Junit Project:\", accuracyJunit)\n",
    "print(\"Accuracy (c-why and c-what) for Junit Project:\", c_why_what_accuracyJunit)\n",
    "print(\"Training set shape for Junit Project:\", junitTrain.shape)\n",
    "print(\"Test set shape for Junit Project:\", junitTest.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Overall for Okhttp Project:\", accuracyOkhttp)\n",
    "print(\"Accuracy (c-why and c-what) for Okhttp Project:\", c_why_what_accuracyOkhttp)\n",
    "print(\"Training set shape for Okhttp Project:\", okhttpTrain.shape)\n",
    "print(\"Test set shape for Okhttp Project:\", okhttpTest.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Overall for Retrofit Project:\", accuracyRetrofit)\n",
    "print(\"Accuracy (c-why and c-what) for Retrofit Project:\", c_why_what_accuracyRetrofit)\n",
    "print(\"Training set shape for Retrofit Project:\", retrofitTest.shape)\n",
    "print(\"Test set shape for Retrofit Project:\", retrofitTrain.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Overall for Springboot Project:\", accuracySpringboot)\n",
    "print(\"Accuracy (c-why and c-what) for Springboot Project:\", c_why_what_accuracySpringboot)\n",
    "print(\"Training set shape for Springboot Project:\", springBootTest.shape)\n",
    "print(\"Test set shape for Springboot Project:\", springBootTrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554b8be",
   "metadata": {},
   "source": [
    "## BI-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7627085b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 - 3s - loss: 0.4238 - accuracy: 0.0432 - 3s/epoch - 318ms/step\n",
      "Epoch 2/10\n",
      "9/9 - 1s - loss: 0.1532 - accuracy: 0.0432 - 602ms/epoch - 67ms/step\n",
      "Epoch 3/10\n",
      "9/9 - 1s - loss: 0.0484 - accuracy: 0.0432 - 585ms/epoch - 65ms/step\n",
      "Epoch 4/10\n",
      "9/9 - 1s - loss: -2.8948e-01 - accuracy: 0.0432 - 575ms/epoch - 64ms/step\n",
      "Epoch 5/10\n",
      "9/9 - 1s - loss: -8.4177e-01 - accuracy: 0.0432 - 574ms/epoch - 64ms/step\n",
      "Epoch 6/10\n",
      "9/9 - 1s - loss: -1.9994e+00 - accuracy: 0.0432 - 581ms/epoch - 65ms/step\n",
      "Epoch 7/10\n",
      "9/9 - 1s - loss: -3.9140e+00 - accuracy: 0.4820 - 585ms/epoch - 65ms/step\n",
      "Epoch 8/10\n",
      "9/9 - 1s - loss: -6.3191e+00 - accuracy: 0.5540 - 581ms/epoch - 65ms/step\n",
      "Epoch 9/10\n",
      "9/9 - 1s - loss: -8.3913e+00 - accuracy: 0.5935 - 578ms/epoch - 64ms/step\n",
      "Epoch 10/10\n",
      "9/9 - 1s - loss: -9.3945e+00 - accuracy: 0.5468 - 584ms/epoch - 65ms/step\n",
      "Epoch 1/10\n",
      "9/9 - 2s - loss: 0.2160 - accuracy: 0.1099 - 2s/epoch - 255ms/step\n",
      "Epoch 2/10\n",
      "9/9 - 1s - loss: -2.3674e+00 - accuracy: 0.0623 - 572ms/epoch - 64ms/step\n",
      "Epoch 3/10\n",
      "9/9 - 1s - loss: -4.1009e+00 - accuracy: 0.0623 - 575ms/epoch - 64ms/step\n",
      "Epoch 4/10\n",
      "9/9 - 1s - loss: -5.7352e+00 - accuracy: 0.0733 - 574ms/epoch - 64ms/step\n",
      "Epoch 5/10\n",
      "9/9 - 1s - loss: -6.7790e+00 - accuracy: 0.0733 - 568ms/epoch - 63ms/step\n",
      "Epoch 6/10\n",
      "9/9 - 1s - loss: -7.6402e+00 - accuracy: 0.0733 - 578ms/epoch - 64ms/step\n",
      "Epoch 7/10\n",
      "9/9 - 1s - loss: -8.8712e+00 - accuracy: 0.0733 - 571ms/epoch - 63ms/step\n",
      "Epoch 8/10\n",
      "9/9 - 1s - loss: -1.0379e+01 - accuracy: 0.0733 - 576ms/epoch - 64ms/step\n",
      "Epoch 9/10\n",
      "9/9 - 1s - loss: -1.3394e+01 - accuracy: 0.1795 - 574ms/epoch - 64ms/step\n",
      "Epoch 10/10\n",
      "9/9 - 1s - loss: -1.6686e+01 - accuracy: 0.3919 - 571ms/epoch - 63ms/step\n",
      "Epoch 1/10\n",
      "9/9 - 35s - loss: 0.1564 - accuracy: 0.0244 - 35s/epoch - 4s/step\n",
      "Epoch 2/10\n",
      "9/9 - 1s - loss: -9.8850e-01 - accuracy: 0.0105 - 668ms/epoch - 74ms/step\n",
      "Epoch 3/10\n",
      "9/9 - 1s - loss: -1.8437e+00 - accuracy: 0.0209 - 621ms/epoch - 69ms/step\n",
      "Epoch 4/10\n",
      "9/9 - 1s - loss: -2.8474e+00 - accuracy: 0.0244 - 599ms/epoch - 67ms/step\n",
      "Epoch 5/10\n",
      "9/9 - 1s - loss: -5.2748e+00 - accuracy: 0.1742 - 674ms/epoch - 75ms/step\n",
      "Epoch 6/10\n",
      "9/9 - 1s - loss: -8.9603e+00 - accuracy: 0.4634 - 645ms/epoch - 72ms/step\n",
      "Epoch 7/10\n",
      "9/9 - 1s - loss: -1.0859e+01 - accuracy: 0.4948 - 745ms/epoch - 83ms/step\n",
      "Epoch 8/10\n",
      "9/9 - 1s - loss: -1.2492e+01 - accuracy: 0.5122 - 816ms/epoch - 91ms/step\n",
      "Epoch 9/10\n",
      "9/9 - 1s - loss: -1.3670e+01 - accuracy: 0.5261 - 728ms/epoch - 81ms/step\n",
      "Epoch 10/10\n",
      "9/9 - 1s - loss: -1.5048e+01 - accuracy: 0.5226 - 644ms/epoch - 72ms/step\n",
      "Epoch 1/10\n",
      "8/8 - 3s - loss: -1.8520e-01 - accuracy: 0.0292 - 3s/epoch - 361ms/step\n",
      "Epoch 2/10\n",
      "8/8 - 1s - loss: -3.9121e+00 - accuracy: 0.0292 - 616ms/epoch - 77ms/step\n",
      "Epoch 3/10\n",
      "8/8 - 1s - loss: -6.6576e+00 - accuracy: 0.0292 - 528ms/epoch - 66ms/step\n",
      "Epoch 4/10\n",
      "8/8 - 1s - loss: -8.8590e+00 - accuracy: 0.0292 - 601ms/epoch - 75ms/step\n",
      "Epoch 5/10\n",
      "8/8 - 1s - loss: -1.0259e+01 - accuracy: 0.0292 - 527ms/epoch - 66ms/step\n",
      "Epoch 6/10\n",
      "8/8 - 1s - loss: -1.1360e+01 - accuracy: 0.0292 - 864ms/epoch - 108ms/step\n",
      "Epoch 7/10\n",
      "8/8 - 1s - loss: -1.2272e+01 - accuracy: 0.0292 - 544ms/epoch - 68ms/step\n",
      "Epoch 8/10\n",
      "8/8 - 1s - loss: -1.3045e+01 - accuracy: 0.0292 - 532ms/epoch - 67ms/step\n",
      "Epoch 9/10\n",
      "8/8 - 1s - loss: -1.3852e+01 - accuracy: 0.0292 - 546ms/epoch - 68ms/step\n",
      "Epoch 10/10\n",
      "8/8 - 1s - loss: -1.4578e+01 - accuracy: 0.0292 - 542ms/epoch - 68ms/step\n",
      "Epoch 1/10\n",
      "10/10 - 2s - loss: 0.6560 - accuracy: 0.7312 - 2s/epoch - 237ms/step\n",
      "Epoch 2/10\n",
      "10/10 - 1s - loss: 0.5229 - accuracy: 0.8250 - 631ms/epoch - 63ms/step\n",
      "Epoch 3/10\n",
      "10/10 - 1s - loss: 0.2497 - accuracy: 0.8188 - 621ms/epoch - 62ms/step\n",
      "Epoch 4/10\n",
      "10/10 - 1s - loss: -1.2233e-01 - accuracy: 0.7781 - 623ms/epoch - 62ms/step\n",
      "Epoch 5/10\n",
      "10/10 - 1s - loss: -4.7066e-01 - accuracy: 0.8188 - 623ms/epoch - 62ms/step\n",
      "Epoch 6/10\n",
      "10/10 - 1s - loss: -9.8035e-01 - accuracy: 0.7969 - 618ms/epoch - 62ms/step\n",
      "Epoch 7/10\n",
      "10/10 - 1s - loss: -1.6132e+00 - accuracy: 0.8469 - 609ms/epoch - 61ms/step\n",
      "Epoch 8/10\n",
      "10/10 - 1s - loss: -2.0396e+00 - accuracy: 0.8250 - 612ms/epoch - 61ms/step\n",
      "Epoch 9/10\n",
      "10/10 - 1s - loss: -2.9600e+00 - accuracy: 0.8531 - 605ms/epoch - 60ms/step\n",
      "Epoch 10/10\n",
      "10/10 - 1s - loss: -3.4721e+00 - accuracy: 0.8625 - 612ms/epoch - 61ms/step\n",
      "2/2 [==============================] - 1s 14ms/step\n",
      "2/2 [==============================] - 1s 13ms/step\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3881f2c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3881f2c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 16ms/step\n",
      "WARNING:tensorflow:6 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3881f0160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3881f0160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 19ms/step\n",
      "2/2 [==============================] - 1s 16ms/step\n",
      "Accuracy Overall for Apache Project: 0.32\n",
      "Accuracy Overall for Junit Project: 0.30612244897959184\n",
      "Accuracy Overall for Okhttp Project: 0.49019607843137253\n",
      "Accuracy Overall for Retrofit Project: 0.0\n",
      "Accuracy Overall for Springboot Project: 0.7719298245614035\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the maximum number of words to consider in the text\n",
    "MAX_WORDS = 10000\n",
    "# Set the maximum length of each input sequence\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Convert the text data into sequences of integers\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(dfApache['new_message1'])\n",
    "tokenizer.fit_on_texts(dfJunit['new_message1'])\n",
    "tokenizer.fit_on_texts(dfOkhttp['new_message1'])\n",
    "tokenizer.fit_on_texts(dfRetrofit['new_message1'])\n",
    "tokenizer.fit_on_texts(dfSpringBoot['new_message1'])\n",
    "\n",
    "sequencesApache = tokenizer.texts_to_sequences(dfApache['new_message1'])\n",
    "sequencesJunit = tokenizer.texts_to_sequences(dfJunit['new_message1'])\n",
    "sequencesOkhttp = tokenizer.texts_to_sequences(dfOkhttp['new_message1'])\n",
    "sequencesRetrofit = tokenizer.texts_to_sequences(dfRetrofit['new_message1'])\n",
    "sequencesSpringBoot = tokenizer.texts_to_sequences(dfSpringBoot['new_message1'])\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "padded_sequencesApache = pad_sequences(sequencesApache, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_sequencesJunit = pad_sequences(sequencesJunit, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_sequencesOkhttp = pad_sequences(sequencesOkhttp, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_sequencesRetrofit = pad_sequences(sequencesRetrofit, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_sequencesSpringBoot = pad_sequences(sequencesSpringBoot, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_trainApache, X_testApache, y_trainApache, y_testApache = train_test_split(\n",
    "    padded_sequencesApache, dfApache[label_column], test_size=0.15, random_state=42)\n",
    "X_trainJunit, X_testJunit, y_trainJunit, y_testJunit = train_test_split(\n",
    "    padded_sequencesJunit, dfJunit[label_column], test_size=0.15, random_state=42)\n",
    "X_trainOkhttp, X_testOkhttp, y_trainOkhttp, y_testOkhttp = train_test_split(\n",
    "    padded_sequencesOkhttp, dfOkhttp[label_column], test_size=0.15, random_state=42)\n",
    "X_trainRetrofit, X_testRetrofit, y_trainRetrofit, y_testRetrofit = train_test_split(\n",
    "    padded_sequencesRetrofit, dfRetrofit[label_column], test_size=0.15, random_state=42)\n",
    "X_trainSpringboot, X_testSpringboot, y_trainSpringboot, y_testSpringboot = train_test_split(\n",
    "    padded_sequencesSpringBoot, dfSpringBoot[label_column], test_size=0.15, random_state=42)\n",
    "\n",
    "# Define the Bi-LSTM model\n",
    "modelApache = Sequential()\n",
    "modelApache.add(Embedding(MAX_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "modelApache.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "modelApache.add(Bidirectional(LSTM(64)))\n",
    "modelApache.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "modelJunit = Sequential()\n",
    "modelJunit.add(Embedding(MAX_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "modelJunit.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "modelJunit.add(Bidirectional(LSTM(64)))\n",
    "modelJunit.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "modelOkhttp = Sequential()\n",
    "modelOkhttp.add(Embedding(MAX_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "modelOkhttp.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "modelOkhttp.add(Bidirectional(LSTM(64)))\n",
    "modelOkhttp.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "modelRetrofit = Sequential()\n",
    "modelRetrofit.add(Embedding(MAX_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "modelRetrofit.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "modelRetrofit.add(Bidirectional(LSTM(64)))\n",
    "modelRetrofit.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "modelSpringboot = Sequential()\n",
    "modelSpringboot.add(Embedding(MAX_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "modelSpringboot.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "modelSpringboot.add(Bidirectional(LSTM(64)))\n",
    "modelSpringboot.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the models\n",
    "modelApache.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelJunit.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelOkhttp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelRetrofit.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelSpringboot.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the models\n",
    "modelApache.fit(X_trainApache, y_trainApache, epochs=10, batch_size=32, verbose=2)\n",
    "modelJunit.fit(X_trainJunit, y_trainJunit, epochs=10, batch_size=32, verbose=2)\n",
    "modelOkhttp.fit(X_trainOkhttp, y_trainOkhttp, epochs=10, batch_size=32, verbose=2)\n",
    "modelRetrofit.fit(X_trainRetrofit, y_trainRetrofit, epochs=10, batch_size=32, verbose=2)\n",
    "modelSpringboot.fit(X_trainSpringboot, y_trainSpringboot, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# Evaluate the models on the test sets\n",
    "predictionsApache = (modelApache.predict(X_testApache) > 0.5).astype(int)\n",
    "accuracyApache = accuracy_score(y_testApache, predictionsApache)\n",
    "\n",
    "predictionsJunit = (modelJunit.predict(X_testJunit) > 0.5).astype(int)\n",
    "accuracyJunit = accuracy_score(y_testJunit, predictionsJunit)\n",
    "\n",
    "predictionsOkhttp = (modelOkhttp.predict(X_testOkhttp) > 0.5).astype(int)\n",
    "accuracyOkhttp = accuracy_score(y_testOkhttp, predictionsOkhttp)\n",
    "\n",
    "predictionsRetrofit = (modelRetrofit.predict(X_testRetrofit) > 0.5).astype(int)\n",
    "accuracyRetrofit = accuracy_score(y_testRetrofit, predictionsRetrofit)\n",
    "\n",
    "predictionsSpringboot = (modelSpringboot.predict(X_testSpringboot) > 0.5).astype(int)\n",
    "accuracySpringboot = accuracy_score(y_testSpringboot, predictionsSpringboot)\n",
    "\n",
    "# Display the results\n",
    "print(\"Accuracy Overall for Apache Project:\", accuracyApache)\n",
    "print(\"Accuracy Overall for Junit Project:\", accuracyJunit)\n",
    "print(\"Accuracy Overall for Okhttp Project:\", accuracyOkhttp)\n",
    "print(\"Accuracy Overall for Retrofit Project:\", accuracyRetrofit)\n",
    "print(\"Accuracy Overall for Springboot Project:\", accuracySpringboot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29c8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab44191b",
   "metadata": {},
   "source": [
    "# BERT + BI-LSTM MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5a1a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 75s - loss: 0.1163 - binary_accuracy: 0.0432 - 75s/epoch - 8s/step\n",
      "Epoch 2/20\n",
      "9/9 - 66s - loss: -1.9324e-01 - binary_accuracy: 0.0576 - 66s/epoch - 7s/step\n",
      "Epoch 3/20\n",
      "9/9 - 65s - loss: -4.2770e-01 - binary_accuracy: 0.0791 - 65s/epoch - 7s/step\n",
      "Epoch 4/20\n",
      "9/9 - 67s - loss: -9.0359e-01 - binary_accuracy: 0.1187 - 67s/epoch - 7s/step\n",
      "Epoch 5/20\n",
      "9/9 - 67s - loss: -1.4522e+00 - binary_accuracy: 0.1942 - 67s/epoch - 7s/step\n",
      "Epoch 6/20\n",
      "9/9 - 67s - loss: -2.5446e+00 - binary_accuracy: 0.1583 - 67s/epoch - 7s/step\n",
      "Epoch 7/20\n",
      "9/9 - 68s - loss: -3.6520e+00 - binary_accuracy: 0.2626 - 68s/epoch - 8s/step\n",
      "Epoch 8/20\n",
      "9/9 - 71s - loss: -5.0277e+00 - binary_accuracy: 0.3094 - 71s/epoch - 8s/step\n",
      "Epoch 9/20\n",
      "9/9 - 72s - loss: -5.9820e+00 - binary_accuracy: 0.3094 - 72s/epoch - 8s/step\n",
      "Epoch 10/20\n",
      "9/9 - 67s - loss: -7.1351e+00 - binary_accuracy: 0.3921 - 67s/epoch - 7s/step\n",
      "Epoch 11/20\n",
      "9/9 - 70s - loss: -7.3081e+00 - binary_accuracy: 0.4640 - 70s/epoch - 8s/step\n",
      "Epoch 12/20\n",
      "9/9 - 70s - loss: -8.3559e+00 - binary_accuracy: 0.3417 - 70s/epoch - 8s/step\n",
      "Epoch 13/20\n",
      "9/9 - 72s - loss: -8.1667e+00 - binary_accuracy: 0.5863 - 72s/epoch - 8s/step\n",
      "Epoch 14/20\n",
      "9/9 - 76s - loss: -8.3103e+00 - binary_accuracy: 0.2986 - 76s/epoch - 8s/step\n",
      "Epoch 15/20\n",
      "9/9 - 75s - loss: -9.0920e+00 - binary_accuracy: 0.3705 - 75s/epoch - 8s/step\n",
      "Epoch 16/20\n",
      "9/9 - 78s - loss: -9.6077e+00 - binary_accuracy: 0.4568 - 78s/epoch - 9s/step\n",
      "Epoch 17/20\n",
      "9/9 - 82s - loss: -1.0541e+01 - binary_accuracy: 0.5144 - 82s/epoch - 9s/step\n",
      "Epoch 18/20\n",
      "9/9 - 86s - loss: -1.1155e+01 - binary_accuracy: 0.5612 - 86s/epoch - 10s/step\n",
      "Epoch 19/20\n",
      "9/9 - 85s - loss: -1.1591e+01 - binary_accuracy: 0.5719 - 85s/epoch - 9s/step\n",
      "Epoch 20/20\n",
      "9/9 - 84s - loss: -1.2162e+01 - binary_accuracy: 0.6007 - 84s/epoch - 9s/step\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 97s - loss: -1.0206e+01 - binary_accuracy: 0.3757 - 97s/epoch - 11s/step\n",
      "Epoch 2/20\n",
      "9/9 - 106s - loss: -1.1046e+01 - binary_accuracy: 0.0916 - 106s/epoch - 12s/step\n",
      "Epoch 3/20\n",
      "9/9 - 100s - loss: -1.1571e+01 - binary_accuracy: 0.1172 - 100s/epoch - 11s/step\n",
      "Epoch 4/20\n",
      "9/9 - 89s - loss: -1.2013e+01 - binary_accuracy: 0.1538 - 89s/epoch - 10s/step\n",
      "Epoch 5/20\n",
      "9/9 - 84s - loss: -1.2257e+01 - binary_accuracy: 0.1502 - 84s/epoch - 9s/step\n",
      "Epoch 6/20\n",
      "9/9 - 82s - loss: -1.2645e+01 - binary_accuracy: 0.1795 - 82s/epoch - 9s/step\n",
      "Epoch 7/20\n",
      "9/9 - 76s - loss: -1.2997e+01 - binary_accuracy: 0.1722 - 76s/epoch - 8s/step\n",
      "Epoch 8/20\n",
      "9/9 - 76s - loss: -1.2942e+01 - binary_accuracy: 0.1941 - 76s/epoch - 8s/step\n",
      "Epoch 9/20\n",
      "9/9 - 68s - loss: -1.3696e+01 - binary_accuracy: 0.2088 - 68s/epoch - 8s/step\n",
      "Epoch 10/20\n",
      "9/9 - 87s - loss: -1.4165e+01 - binary_accuracy: 0.2161 - 87s/epoch - 10s/step\n",
      "Epoch 11/20\n",
      "9/9 - 79s - loss: -1.4450e+01 - binary_accuracy: 0.2418 - 79s/epoch - 9s/step\n",
      "Epoch 12/20\n",
      "9/9 - 84s - loss: -1.4444e+01 - binary_accuracy: 0.2454 - 84s/epoch - 9s/step\n",
      "Epoch 13/20\n",
      "9/9 - 83s - loss: -1.4630e+01 - binary_accuracy: 0.2271 - 83s/epoch - 9s/step\n",
      "Epoch 14/20\n",
      "9/9 - 83s - loss: -1.4912e+01 - binary_accuracy: 0.2491 - 83s/epoch - 9s/step\n",
      "Epoch 15/20\n",
      "9/9 - 79s - loss: -1.4457e+01 - binary_accuracy: 0.2088 - 79s/epoch - 9s/step\n",
      "Epoch 16/20\n",
      "9/9 - 75s - loss: -1.5178e+01 - binary_accuracy: 0.2747 - 75s/epoch - 8s/step\n",
      "Epoch 17/20\n",
      "9/9 - 72s - loss: -1.5708e+01 - binary_accuracy: 0.2967 - 72s/epoch - 8s/step\n",
      "Epoch 18/20\n",
      "9/9 - 69s - loss: -1.5826e+01 - binary_accuracy: 0.3077 - 69s/epoch - 8s/step\n",
      "Epoch 19/20\n",
      "9/9 - 68s - loss: -1.5554e+01 - binary_accuracy: 0.2418 - 68s/epoch - 8s/step\n",
      "Epoch 20/20\n",
      "9/9 - 74s - loss: -1.6132e+01 - binary_accuracy: 0.2930 - 74s/epoch - 8s/step\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 91s - loss: -8.3681e+00 - binary_accuracy: 0.2661 - 91s/epoch - 10s/step\n",
      "Epoch 2/20\n",
      "9/9 - 77s - loss: -7.6556e+00 - binary_accuracy: 0.1533 - 77s/epoch - 9s/step\n",
      "Epoch 3/20\n",
      "9/9 - 74s - loss: -1.1081e+01 - binary_accuracy: 0.3624 - 74s/epoch - 8s/step\n",
      "Epoch 4/20\n",
      "9/9 - 77s - loss: -1.0462e+01 - binary_accuracy: 0.4251 - 77s/epoch - 9s/step\n",
      "Epoch 5/20\n",
      "9/9 - 78s - loss: -1.1653e+01 - binary_accuracy: 0.2195 - 78s/epoch - 9s/step\n",
      "Epoch 6/20\n",
      "9/9 - 76s - loss: -1.1590e+01 - binary_accuracy: 0.2230 - 76s/epoch - 8s/step\n",
      "Epoch 7/20\n",
      "9/9 - 76s - loss: -1.2328e+01 - binary_accuracy: 0.3833 - 76s/epoch - 8s/step\n",
      "Epoch 8/20\n",
      "9/9 - 73s - loss: -1.2471e+01 - binary_accuracy: 0.4390 - 73s/epoch - 8s/step\n",
      "Epoch 9/20\n",
      "9/9 - 78s - loss: -1.2703e+01 - binary_accuracy: 0.4460 - 78s/epoch - 9s/step\n",
      "Epoch 10/20\n",
      "9/9 - 75s - loss: -1.3161e+01 - binary_accuracy: 0.3902 - 75s/epoch - 8s/step\n",
      "Epoch 11/20\n",
      "9/9 - 74s - loss: -1.3186e+01 - binary_accuracy: 0.4216 - 74s/epoch - 8s/step\n",
      "Epoch 12/20\n",
      "9/9 - 72s - loss: -1.3630e+01 - binary_accuracy: 0.4390 - 72s/epoch - 8s/step\n",
      "Epoch 13/20\n",
      "9/9 - 71s - loss: -1.3615e+01 - binary_accuracy: 0.4704 - 71s/epoch - 8s/step\n",
      "Epoch 14/20\n",
      "9/9 - 71s - loss: -1.3710e+01 - binary_accuracy: 0.4425 - 71s/epoch - 8s/step\n",
      "Epoch 15/20\n",
      "9/9 - 73s - loss: -1.3892e+01 - binary_accuracy: 0.4460 - 73s/epoch - 8s/step\n",
      "Epoch 16/20\n",
      "9/9 - 76s - loss: -1.3233e+01 - binary_accuracy: 0.4774 - 76s/epoch - 8s/step\n",
      "Epoch 17/20\n",
      "9/9 - 77s - loss: -1.2129e+01 - binary_accuracy: 0.2718 - 77s/epoch - 9s/step\n",
      "Epoch 18/20\n",
      "9/9 - 99s - loss: -1.3832e+01 - binary_accuracy: 0.4390 - 99s/epoch - 11s/step\n",
      "Epoch 19/20\n",
      "9/9 - 111s - loss: -1.4260e+01 - binary_accuracy: 0.4983 - 111s/epoch - 12s/step\n",
      "Epoch 20/20\n",
      "9/9 - 102s - loss: -1.4272e+01 - binary_accuracy: 0.4669 - 102s/epoch - 11s/step\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 98s - loss: -1.8126e+01 - binary_accuracy: 0.2808 - 98s/epoch - 12s/step\n",
      "Epoch 2/20\n",
      "8/8 - 84s - loss: -1.7985e+01 - binary_accuracy: 0.0500 - 84s/epoch - 11s/step\n",
      "Epoch 3/20\n",
      "8/8 - 80s - loss: -1.8364e+01 - binary_accuracy: 0.0500 - 80s/epoch - 10s/step\n",
      "Epoch 4/20\n",
      "8/8 - 78s - loss: -1.8567e+01 - binary_accuracy: 0.0667 - 78s/epoch - 10s/step\n",
      "Epoch 5/20\n",
      "8/8 - 82s - loss: -1.8750e+01 - binary_accuracy: 0.0792 - 82s/epoch - 10s/step\n",
      "Epoch 6/20\n",
      "8/8 - 91s - loss: -1.8877e+01 - binary_accuracy: 0.0833 - 91s/epoch - 11s/step\n",
      "Epoch 7/20\n",
      "8/8 - 86s - loss: -1.8933e+01 - binary_accuracy: 0.0792 - 86s/epoch - 11s/step\n",
      "Epoch 8/20\n",
      "8/8 - 94s - loss: -1.9434e+01 - binary_accuracy: 0.1083 - 94s/epoch - 12s/step\n",
      "Epoch 9/20\n",
      "8/8 - 105s - loss: -2.0244e+01 - binary_accuracy: 0.1167 - 105s/epoch - 13s/step\n",
      "Epoch 10/20\n",
      "8/8 - 97s - loss: -2.0540e+01 - binary_accuracy: 0.1333 - 97s/epoch - 12s/step\n",
      "Epoch 11/20\n",
      "8/8 - 113s - loss: -2.0744e+01 - binary_accuracy: 0.1625 - 113s/epoch - 14s/step\n",
      "Epoch 12/20\n",
      "8/8 - 109s - loss: -2.2250e+01 - binary_accuracy: 0.1917 - 109s/epoch - 14s/step\n",
      "Epoch 13/20\n",
      "8/8 - 102s - loss: -2.1952e+01 - binary_accuracy: 0.1917 - 102s/epoch - 13s/step\n",
      "Epoch 14/20\n",
      "8/8 - 99s - loss: -2.2139e+01 - binary_accuracy: 0.2292 - 99s/epoch - 12s/step\n",
      "Epoch 15/20\n",
      "8/8 - 101s - loss: -2.2413e+01 - binary_accuracy: 0.2333 - 101s/epoch - 13s/step\n",
      "Epoch 16/20\n",
      "8/8 - 103s - loss: -2.3253e+01 - binary_accuracy: 0.2583 - 103s/epoch - 13s/step\n",
      "Epoch 17/20\n",
      "8/8 - 106s - loss: -2.2502e+01 - binary_accuracy: 0.2792 - 106s/epoch - 13s/step\n",
      "Epoch 18/20\n",
      "8/8 - 116s - loss: -2.3817e+01 - binary_accuracy: 0.2000 - 116s/epoch - 15s/step\n",
      "Epoch 19/20\n",
      "8/8 - 115s - loss: -2.4944e+01 - binary_accuracy: 0.3042 - 115s/epoch - 14s/step\n",
      "Epoch 20/20\n",
      "8/8 - 116s - loss: -2.5323e+01 - binary_accuracy: 0.3250 - 116s/epoch - 14s/step\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 137s - loss: -1.0356e+00 - binary_accuracy: 0.6071 - 137s/epoch - 14s/step\n",
      "Epoch 2/20\n",
      "10/10 - 135s - loss: -1.3107e+00 - binary_accuracy: 0.8313 - 135s/epoch - 13s/step\n",
      "Epoch 3/20\n",
      "10/10 - 154s - loss: -1.0397e+00 - binary_accuracy: 0.8438 - 154s/epoch - 15s/step\n",
      "Epoch 4/20\n",
      "10/10 - 146s - loss: -5.6235e-01 - binary_accuracy: 0.8531 - 146s/epoch - 15s/step\n",
      "Epoch 5/20\n",
      "10/10 - 139s - loss: -8.4160e-01 - binary_accuracy: 0.8438 - 139s/epoch - 14s/step\n",
      "Epoch 6/20\n",
      "10/10 - 153s - loss: -1.8498e+00 - binary_accuracy: 0.8438 - 153s/epoch - 15s/step\n",
      "Epoch 7/20\n",
      "10/10 - 135s - loss: -1.5834e+00 - binary_accuracy: 0.8156 - 135s/epoch - 13s/step\n",
      "Epoch 8/20\n",
      "10/10 - 134s - loss: -1.7716e+00 - binary_accuracy: 0.8156 - 134s/epoch - 13s/step\n",
      "Epoch 9/20\n",
      "10/10 - 158s - loss: -1.6648e+00 - binary_accuracy: 0.8344 - 158s/epoch - 16s/step\n",
      "Epoch 10/20\n",
      "10/10 - 165s - loss: -1.9085e+00 - binary_accuracy: 0.7937 - 165s/epoch - 17s/step\n",
      "Epoch 11/20\n",
      "10/10 - 198s - loss: -2.4425e+00 - binary_accuracy: 0.7969 - 198s/epoch - 20s/step\n",
      "Epoch 12/20\n",
      "10/10 - 186s - loss: -2.4595e+00 - binary_accuracy: 0.7719 - 186s/epoch - 19s/step\n",
      "Epoch 13/20\n",
      "10/10 - 206s - loss: -2.2317e+00 - binary_accuracy: 0.8469 - 206s/epoch - 21s/step\n",
      "Epoch 14/20\n",
      "10/10 - 197s - loss: -2.2765e+00 - binary_accuracy: 0.7656 - 197s/epoch - 20s/step\n",
      "Epoch 15/20\n",
      "10/10 - 195s - loss: -2.9049e+00 - binary_accuracy: 0.8094 - 195s/epoch - 19s/step\n",
      "Epoch 16/20\n",
      "10/10 - 151s - loss: -3.1280e+00 - binary_accuracy: 0.8188 - 151s/epoch - 15s/step\n",
      "Epoch 17/20\n",
      "10/10 - 106s - loss: -3.3210e+00 - binary_accuracy: 0.8469 - 106s/epoch - 11s/step\n",
      "Epoch 18/20\n",
      "10/10 - 100s - loss: -2.6957e+00 - binary_accuracy: 0.7781 - 100s/epoch - 10s/step\n",
      "Epoch 19/20\n",
      "10/10 - 100s - loss: -3.4784e+00 - binary_accuracy: 0.8625 - 100s/epoch - 10s/step\n",
      "Epoch 20/20\n",
      "10/10 - 101s - loss: -3.7460e+00 - binary_accuracy: 0.8281 - 101s/epoch - 10s/step\n",
      "2/2 [==============================] - 8s 1s/step\n",
      "2/2 [==============================] - 5s 1s/step\n",
      "2/2 [==============================] - 5s 2s/step\n",
      "2/2 [==============================] - 5s 992ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'modelSpringBoott' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m predictionsRetrofit \u001b[38;5;241m=\u001b[39m (modelRetrofit\u001b[38;5;241m.\u001b[39mpredict(X_testRetrofit) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    112\u001b[0m accuracyRetrofit \u001b[38;5;241m=\u001b[39m accuracy_score(y_testRetrofit, predictionsRetrofit)\n\u001b[0;32m--> 113\u001b[0m predictionsSpringBoot \u001b[38;5;241m=\u001b[39m (\u001b[43mmodelSpringBoott\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_testSpringBoot) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    114\u001b[0m accuracySpringBoot \u001b[38;5;241m=\u001b[39m accuracy_score(y_testSpringBoot, predictionsSpringBoot)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelSpringBoott' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the maximum sequence length and other constants\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "LABEL_COLUMN = 'label'\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert the text data into BERT input format\n",
    "\n",
    "\n",
    "input_sequencesApache = tokenizer.batch_encode_plus(dfApache['new_message1'].tolist(), \n",
    "                                                    padding='max_length', \n",
    "                                                    truncation=True,\n",
    "                                                    max_length=MAX_SEQUENCE_LENGTH,\n",
    "                                                    return_tensors='tf')\n",
    "input_sequencesJunit = tokenizer.batch_encode_plus(dfJunit['new_message1'].tolist(), \n",
    "                                                    padding='max_length', \n",
    "                                                    truncation=True,\n",
    "                                                    max_length=MAX_SEQUENCE_LENGTH,\n",
    "                                                    return_tensors='tf')\n",
    "input_sequencesOkhttp = tokenizer.batch_encode_plus(dfOkhttp['new_message1'].tolist(), \n",
    "                                                    padding='max_length', \n",
    "                                                    truncation=True,\n",
    "                                                    max_length=MAX_SEQUENCE_LENGTH,\n",
    "                                                    return_tensors='tf')\n",
    "input_sequencesRetrofit = tokenizer.batch_encode_plus(dfRetrofit['new_message1'].tolist(), \n",
    "                                                    padding='max_length', \n",
    "                                                    truncation=True,\n",
    "                                                    max_length=MAX_SEQUENCE_LENGTH,\n",
    "                                                    return_tensors='tf')\n",
    "input_sequencesSpringBoot = tokenizer.batch_encode_plus(dfSpringBoot['new_message1'].tolist(), \n",
    "                                                    padding='max_length', \n",
    "                                                    truncation=True,\n",
    "                                                    max_length=MAX_SEQUENCE_LENGTH,\n",
    "                                                    return_tensors='tf')\n",
    "\n",
    "\n",
    "\n",
    "input_ids_apache = np.array(input_sequencesApache['input_ids'])\n",
    "\n",
    "input_ids_Junit = np.array(input_sequencesJunit['input_ids'])\n",
    "\n",
    "input_ids_Okhttp = np.array(input_sequencesOkhttp['input_ids'])\n",
    "\n",
    "input_ids_Retrofit = np.array(input_sequencesRetrofit['input_ids'])\n",
    "\n",
    "input_ids_SpringBoot  = np.array(input_sequencesSpringBoot ['input_ids'])\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_trainApache, X_testApache, y_trainApache, y_testApache = train_test_split(\n",
    "    input_ids_apache, dfApache[LABEL_COLUMN], test_size=0.15, random_state=42)\n",
    "X_trainJunit, X_testJunit, y_trainJunit, y_testJunit = train_test_split(\n",
    "    input_ids_Junit, dfJunit[LABEL_COLUMN], test_size=0.15, random_state=42)\n",
    "X_trainOkhttp, X_testOkhttp, y_trainOkhttp, y_testOkhttp = train_test_split(\n",
    "    input_ids_Okhttp, dfOkhttp[LABEL_COLUMN], test_size=0.15, random_state=42)\n",
    "X_trainRetrofit, X_testRetrofit, y_trainRetrofit, y_testRetrofit = train_test_split(\n",
    "    input_ids_Retrofit, dfRetrofit[LABEL_COLUMN], test_size=0.15, random_state=42)\n",
    "X_trainSpringBoot, X_testSpringBoot, y_trainSpringBoot, y_testSpringBoot = train_test_split(\n",
    "    input_ids_SpringBoot, dfSpringBoot[LABEL_COLUMN], test_size=0.15, random_state=42)\n",
    "# Define the BERT model\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "bert_output = bert_model(input_layer)[0]\n",
    "pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(bert_output)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(pooling_layer)\n",
    "\n",
    "\n",
    "modelApache = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "modelJunit = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "modelOkhttp = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "modelRetrofit = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "modelSpringBoot = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model with the appropriate optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "modelApache.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "modelJunit.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "modelOkhttp.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "modelRetrofit.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "modelSpringBoot.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the model\n",
    "modelApache.fit(X_trainApache, y_trainApache, epochs=20, batch_size=32, verbose=2)\n",
    "modelJunit.fit(X_trainJunit, y_trainJunit, epochs=20, batch_size=32, verbose=2)\n",
    "modelOkhttp.fit(X_trainOkhttp, y_trainOkhttp, epochs=20, batch_size=32, verbose=2)\n",
    "modelRetrofit.fit(X_trainRetrofit, y_trainRetrofit, epochs=20, batch_size=32, verbose=2)\n",
    "modelSpringBoot.fit(X_trainSpringBoot, y_trainSpringBoot, epochs=20, batch_size=32, verbose=2)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "\n",
    "predictionsApache = (modelApache.predict(X_testApache) > 0.5).astype(int)\n",
    "accuracyApache = accuracy_score(y_testApache, predictionsApache)\n",
    "predictionsJunit = (modelJunit.predict(X_testJunit) > 0.5).astype(int)\n",
    "accuracyJunit = accuracy_score(y_testJunit, predictionsJunit)\n",
    "predictionsOkhttp = (modelOkhttp.predict(X_testOkhttp) > 0.5).astype(int)\n",
    "accuracyOkhttp = accuracy_score(y_testOkhttp, predictionsOkhttp)\n",
    "predictionsRetrofit = (modelRetrofit.predict(X_testRetrofit) > 0.5).astype(int)\n",
    "accuracyRetrofit = accuracy_score(y_testRetrofit, predictionsRetrofit)\n",
    "predictionsSpringBoot = (modelSpringBoot.predict(X_testSpringBoot) > 0.5).astype(int)\n",
    "accuracySpringBoot = accuracy_score(y_testSpringBoot, predictionsSpringBoot)\n",
    "\n",
    "# Display the result\n",
    "# print(\"Accuracy Overall for Apache Project:\", accuracyApache)\n",
    "# print(\"Accuracy Overall for Junit Project:\", accuracyJunit)\n",
    "# print(\"Accuracy Overall for Okhttp Project:\", accuracyOkhttp)\n",
    "# print(\"Accuracy Overall for Retrofit[ Project:\", accuracyRetrofit)\n",
    "# print(\"Accuracy Overall for SpringBoot Project:\", accuracySpringBoott)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f803993",
   "metadata": {},
   "source": [
    "#### name error solved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4bfa20",
   "metadata": {},
   "source": [
    "# BERT+BI-LSTM MODEL RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2098884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 1s/step\n",
      "2/2 [==============================] - 3s 1s/step\n",
      "2/2 [==============================] - 3s 1s/step\n",
      "2/2 [==============================] - 3s 749ms/step\n",
      "2/2 [==============================] - 5s 2s/step\n",
      "Accuracy Overall for Apache Project: 0.5\n",
      "Accuracy Overall for Junit Project: 0.2857142857142857\n",
      "Accuracy Overall for Okhttp Project: 0.5686274509803921\n",
      "Accuracy Overall for Retrofit[ Project: 0.3953488372093023\n",
      "Accuracy Overall for SpringBoot Project: 0.7017543859649122\n"
     ]
    }
   ],
   "source": [
    "predictionsApache = (modelApache.predict(X_testApache) > 0.5).astype(int)\n",
    "accuracyApache = accuracy_score(y_testApache, predictionsApache)\n",
    "predictionsJunit = (modelJunit.predict(X_testJunit) > 0.5).astype(int)\n",
    "accuracyJunit = accuracy_score(y_testJunit, predictionsJunit)\n",
    "predictionsOkhttp = (modelOkhttp.predict(X_testOkhttp) > 0.5).astype(int)\n",
    "accuracyOkhttp = accuracy_score(y_testOkhttp, predictionsOkhttp)\n",
    "predictionsRetrofit = (modelRetrofit.predict(X_testRetrofit) > 0.5).astype(int)\n",
    "accuracyRetrofit = accuracy_score(y_testRetrofit, predictionsRetrofit)\n",
    "predictionsSpringBoot = (modelSpringBoot.predict(X_testSpringBoot) > 0.5).astype(int)\n",
    "accuracySpringBoot = accuracy_score(y_testSpringBoot, predictionsSpringBoot)\n",
    "print(\"Accuracy Overall for Apache Project:\", accuracyApache)\n",
    "print(\"Accuracy Overall for Junit Project:\", accuracyJunit)\n",
    "print(\"Accuracy Overall for Okhttp Project:\", accuracyOkhttp)\n",
    "print(\"Accuracy Overall for Retrofit[ Project:\", accuracyRetrofit)\n",
    "print(\"Accuracy Overall for SpringBoot Project:\", accuracySpringBoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d42e6",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
