{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cd8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from joblib import dump\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, confusion_matrix, roc_curve, auc, \\\n",
    "    roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "SCORING = {'accuracy': 'accuracy', 'precision': make_scorer(precision_score), 'recall': make_scorer(recall_score),\n",
    "           'f1': make_scorer(f1_score),\n",
    "           'AUC': make_scorer(roc_auc_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5604f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOriginal = pd.read_excel(\"C:/Users/chels/Desktop/sampledMessages.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a15132",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'label'\n",
    "message_column = 'new_message1'\n",
    "\n",
    "def mylog():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logfile = \"./log/\" + str(datetime.datetime.now().month) + \"-\" + str(datetime.datetime.now().day) + \"-\" + str(\n",
    "        datetime.datetime.now().hour) + \"-\" + str(datetime.datetime.now().minute) + \\\n",
    "              os.path.split(__file__)[-1].split(\".\")[0] + '.log'\n",
    "    fileHandler = logging.FileHandler(logfile, mode='w', encoding='UTF-8')\n",
    "    fileHandler.setLevel(logging.NOTSET)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    logger.addHandler(fileHandler)\n",
    "    return logger\n",
    "\n",
    "# Define the preprocess_text function to perform text preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_text)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "dfOriginal['new_message1'] = dfOriginal['new_message1'].apply(preprocess_text)\n",
    "originalTrain, originalTest = train_test_split(dfOriginal, test_size=0.15, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_trainOriginal = vectorizer.fit_transform(originalTrain['new_message1'])\n",
    "X_testOriginal = vectorizer.transform(originalTest['new_message1'])\n",
    "y_trainOriginal = originalTrain[label_column]\n",
    "y_testOriginal = originalTest[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019ced26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_by_grid(grid: GridSearchCV):\n",
    "    print(\"GridSearchCV is complate!\")\n",
    "    accuRank = grid.cv_results_['rank_test_accuracy']\n",
    "    preMean = grid.cv_results_['mean_test_precision']\n",
    "    bestParam = grid.cv_results_['params']\n",
    "    bestIndex = grid.best_index_\n",
    "    i = bestIndex\n",
    "    rank = 1\n",
    "\n",
    "    while preMean[i] < 0.5:\n",
    "        rank += 1\n",
    "        indx = 0\n",
    "        if rank > 20:\n",
    "            break\n",
    "        for num in accuRank:\n",
    "            if num == rank:\n",
    "                i = indx\n",
    "                break\n",
    "            indx += 1\n",
    "    bestIndex = i\n",
    "\n",
    "    res = \"refit by:\" + str(grid.refit) + \" Parameters: \" + str(bestParam[bestIndex])\n",
    "    #logger.info(res)\n",
    "   # print(res)\n",
    "    return bestParam[bestIndex]\n",
    "\n",
    "def KNNClassifier(trainFeatures, trainLabels):\n",
    "    model = KNeighborsClassifier()\n",
    "    fold = KFold(n_splits=10, random_state=5, shuffle=True)\n",
    "    parameter = {'n_neighbors': np.arange(1, 10, 1),\n",
    "                 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                 }\n",
    "    grid = GridSearchCV(estimator=model, param_grid=parameter, cv=fold,\n",
    "                        scoring=SCORING, refit=\"accuracy\", n_jobs=25)\n",
    "    grid.fit(trainFeatures, trainLabels)\n",
    "    bestParameter = get_score_by_grid(grid)\n",
    "    print(\"KNN Best using %s \" % (bestParameter))\n",
    "    model = KNeighborsClassifier(n_neighbors=bestParameter['n_neighbors'], algorithm=bestParameter['algorithm'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa4e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV is complate!\n",
      "KNN Best using {'algorithm': 'auto', 'n_neighbors': 1} \n",
      "Accuracy Overall for Original Dataset: 0.592741935483871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chels\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\chels\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "#Generating model results for each project\n",
    "KNNModelOriginal = KNNClassifier(X_trainOriginal,y_trainOriginal)\n",
    "KNNModelOriginal.fit(X_trainOriginal, y_trainOriginal)\n",
    "predictionsOriginal=KNNModelOriginal.predict(X_testOriginal)\n",
    "accuracyOriginal = accuracy_score(y_testOriginal, predictionsOriginal)\n",
    "print(\"Accuracy Overall for Original Dataset:\", accuracyOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731161ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFClassifier(trainFeatures, trainLabels):\n",
    "    model = RandomForestClassifier()\n",
    "    fold = KFold(n_splits=10, random_state=5, shuffle=True)\n",
    "    tree_param_grid = {'max_features': [28, 150, 768, 772],\n",
    "                       'min_samples_split': [i for i in np.arange(7, 16, 1)],\n",
    "                       'n_estimators': list(range(50, 100, 20))\n",
    "                       }\n",
    "    grid = GridSearchCV(estimator=model, param_grid=tree_param_grid,\n",
    "                        scoring=SCORING, refit=\"accuracy\", n_jobs=25, cv=fold)\n",
    "    grid.fit(trainFeatures, trainLabels)\n",
    "    bestParameter = get_score_by_grid(grid)\n",
    "    print(\"RForest Best using %s \" % (bestParameter))\n",
    "    model = RandomForestClassifier(max_features=bestParameter['max_features'],\n",
    "                                   min_samples_split=bestParameter['min_samples_split'],\n",
    "                                   n_estimators=bestParameter['n_estimators'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3742b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chels\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV is complate!\n",
      "RForest Best using {'max_features': 28, 'min_samples_split': 7, 'n_estimators': 50} \n",
      "Accuracy Overall for Original Dataset: 0.782258064516129\n"
     ]
    }
   ],
   "source": [
    "RandForestModelOriginal = RFClassifier(X_trainOriginal ,y_trainOriginal)\n",
    "RandForestModelOriginal.fit(X_trainOriginal, y_trainOriginal)\n",
    "predictionsOriginal=RandForestModelOriginal.predict(X_testOriginal)\n",
    "accuracyOriginal = accuracy_score(y_testOriginal, predictionsOriginal)\n",
    "print(\"Accuracy Overall for Original Dataset:\", accuracyOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "587bffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRClassifier(trainFeatures, trainLabels):\n",
    "    parameters = {'C': np.linspace(0.0001, 20, 20),\n",
    "                  'random_state': np.arange(1, 5),\n",
    "                  'solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\"],\n",
    "                  'multi_class': ['ovr'],\n",
    "                  'dual': [False],\n",
    "                  'verbose': [False],\n",
    "                  'max_iter': [500]\n",
    "                  }\n",
    "    fold = KFold(n_splits=10, random_state=5, shuffle=True)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        grid = GridSearchCV(LogisticRegression(), parameters, scoring=SCORING, refit=\"accuracy\", cv=fold, n_jobs=25)\n",
    "        grid.fit(trainFeatures, trainLabels)\n",
    "    \n",
    "    bestParameter = get_score_by_grid(grid)\n",
    "    print(\"LR Best: using %s \" % (bestParameter))\n",
    "    model = LogisticRegression(C=bestParameter['C'], random_state=bestParameter['random_state'],\n",
    "                               solver=bestParameter['solver'], multi_class='ovr', dual=False, verbose=False,\n",
    "                               max_iter=500)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21fb147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV is complate!\n",
      "LR Best: using {'C': 0.0001, 'dual': False, 'max_iter': 500, 'multi_class': 'ovr', 'random_state': 1, 'solver': 'newton-cg', 'verbose': False} \n",
      "Accuracy Overall for Original: 0.6209677419354839\n"
     ]
    }
   ],
   "source": [
    "LRModelOriginal = LRClassifier(X_trainOriginal, y_trainOriginal)\n",
    "LRModelOriginal.fit(X_trainOriginal, y_trainOriginal)\n",
    "predictionsOriginal = LRModelOriginal.predict(X_testOriginal)\n",
    "accuracyOriginal = accuracy_score(y_testOriginal, predictionsOriginal)\n",
    "print(\"Accuracy Overall for Original:\", accuracyOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfefedb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 - 10s - loss: -8.6005e-01 - accuracy: 0.0414 - 10s/epoch - 229ms/step\n",
      "Epoch 2/10\n",
      "44/44 - 5s - loss: -6.3010e+00 - accuracy: 0.3579 - 5s/epoch - 113ms/step\n",
      "Epoch 3/10\n",
      "44/44 - 5s - loss: -1.0977e+01 - accuracy: 0.4343 - 5s/epoch - 112ms/step\n",
      "Epoch 4/10\n",
      "44/44 - 5s - loss: -1.5422e+01 - accuracy: 0.5021 - 5s/epoch - 112ms/step\n",
      "Epoch 5/10\n",
      "44/44 - 5s - loss: -1.8946e+01 - accuracy: 0.5643 - 5s/epoch - 112ms/step\n",
      "Epoch 6/10\n",
      "44/44 - 5s - loss: -2.2763e+01 - accuracy: 0.5064 - 5s/epoch - 112ms/step\n",
      "Epoch 7/10\n",
      "44/44 - 5s - loss: -2.4098e+01 - accuracy: 0.4129 - 5s/epoch - 111ms/step\n",
      "Epoch 8/10\n",
      "44/44 - 5s - loss: -3.0157e+01 - accuracy: 0.5407 - 5s/epoch - 111ms/step\n",
      "Epoch 9/10\n",
      "44/44 - 5s - loss: -3.4462e+01 - accuracy: 0.5343 - 5s/epoch - 112ms/step\n",
      "Epoch 10/10\n",
      "44/44 - 5s - loss: -3.8442e+01 - accuracy: 0.5386 - 5s/epoch - 112ms/step\n",
      "8/8 [==============================] - 1s 33ms/step\n",
      "Accuracy Overall for Original Dataset: 0.47580645161290325\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the maximum number of words to consider in the text\n",
    "MAX_WORDS = 10000\n",
    "# Set the maximum length of each input sequence\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Convert the text data into sequences of integers\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(dfOriginal['new_message1'])\n",
    "\n",
    "\n",
    "sequencesOriginal = tokenizer.texts_to_sequences(dfOriginal['new_message1'])\n",
    "\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "padded_sequencesOriginal = pad_sequences(sequencesOriginal, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_trainOriginal, X_testOriginal, y_trainOriginal, y_testOriginal = train_test_split(\n",
    "    padded_sequencesOriginal, dfOriginal[label_column], test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Define the Bi-LSTM model\n",
    "modelOriginal = Sequential()\n",
    "modelOriginal.add(Embedding(MAX_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "modelOriginal.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "modelOriginal.add(Bidirectional(LSTM(64)))\n",
    "modelOriginal.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# Compile the models\n",
    "modelOriginal.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the models\n",
    "modelOriginal.fit(X_trainOriginal, y_trainOriginal, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "\n",
    "# Evaluate the models on the test sets\n",
    "predictionsOriginal = (modelOriginal.predict(X_testOriginal) > 0.5).astype(int)\n",
    "accuracyOriginal = accuracy_score(y_testOriginal, predictionsOriginal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"Accuracy Overall for Original Dataset:\", accuracyOriginal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7be8413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chels\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chels\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:5805: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "44/44 - 400s - loss: -1.5258e+00 - binary_accuracy: 0.0964 - 400s/epoch - 9s/step\n",
      "Epoch 2/20\n",
      "44/44 - 376s - loss: -5.8663e+00 - binary_accuracy: 0.2571 - 376s/epoch - 9s/step\n",
      "Epoch 3/20\n",
      "44/44 - 377s - loss: -8.0767e+00 - binary_accuracy: 0.3250 - 377s/epoch - 9s/step\n",
      "Epoch 4/20\n",
      "44/44 - 589s - loss: -8.7038e+00 - binary_accuracy: 0.3650 - 589s/epoch - 13s/step\n",
      "Epoch 5/20\n",
      "44/44 - 367s - loss: -8.9668e+00 - binary_accuracy: 0.3243 - 367s/epoch - 8s/step\n",
      "Epoch 6/20\n",
      "44/44 - 377s - loss: -8.9828e+00 - binary_accuracy: 0.3364 - 377s/epoch - 9s/step\n",
      "Epoch 7/20\n",
      "44/44 - 378s - loss: -9.4204e+00 - binary_accuracy: 0.3950 - 378s/epoch - 9s/step\n",
      "Epoch 8/20\n",
      "44/44 - 386s - loss: -9.6109e+00 - binary_accuracy: 0.3607 - 386s/epoch - 9s/step\n",
      "Epoch 9/20\n",
      "44/44 - 388s - loss: -9.8830e+00 - binary_accuracy: 0.3871 - 388s/epoch - 9s/step\n",
      "Epoch 10/20\n",
      "44/44 - 397s - loss: -1.0186e+01 - binary_accuracy: 0.3621 - 397s/epoch - 9s/step\n",
      "Epoch 11/20\n",
      "44/44 - 400s - loss: -1.0738e+01 - binary_accuracy: 0.4036 - 400s/epoch - 9s/step\n",
      "Epoch 12/20\n",
      "44/44 - 394s - loss: -1.0653e+01 - binary_accuracy: 0.4114 - 394s/epoch - 9s/step\n",
      "Epoch 13/20\n",
      "44/44 - 389s - loss: -1.1322e+01 - binary_accuracy: 0.3921 - 389s/epoch - 9s/step\n",
      "Epoch 14/20\n",
      "44/44 - 399s - loss: -1.1614e+01 - binary_accuracy: 0.4321 - 399s/epoch - 9s/step\n",
      "Epoch 15/20\n",
      "44/44 - 404s - loss: -1.1859e+01 - binary_accuracy: 0.4264 - 404s/epoch - 9s/step\n",
      "Epoch 16/20\n",
      "44/44 - 390s - loss: -1.2229e+01 - binary_accuracy: 0.4371 - 390s/epoch - 9s/step\n",
      "Epoch 17/20\n",
      "44/44 - 388s - loss: -1.1773e+01 - binary_accuracy: 0.4086 - 388s/epoch - 9s/step\n",
      "Epoch 18/20\n",
      "44/44 - 386s - loss: -1.2290e+01 - binary_accuracy: 0.4543 - 386s/epoch - 9s/step\n",
      "Epoch 19/20\n",
      "44/44 - 389s - loss: -1.2494e+01 - binary_accuracy: 0.4086 - 389s/epoch - 9s/step\n",
      "Epoch 20/20\n",
      "44/44 - 398s - loss: -1.3070e+01 - binary_accuracy: 0.4457 - 398s/epoch - 9s/step\n",
      "8/8 [==============================] - 23s 3s/step\n",
      "Accuracy Overall for Original Dataset: 0.5080645161290323\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "LABEL_COLUMN = 'label'\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "input_sequencesOriginal = tokenizer.batch_encode_plus(dfOriginal['new_message1'].tolist(), \n",
    "                                                    padding='max_length', \n",
    "                                                    truncation=True,\n",
    "                                                    max_length=MAX_SEQUENCE_LENGTH,\n",
    "                                                    return_tensors='tf')\n",
    "\n",
    "input_ids_Original = np.array(input_sequencesOriginal['input_ids'])\n",
    "X_trainOriginal, X_testOriginal, y_trainOriginal, y_testOriginal = train_test_split(\n",
    "    input_ids_Original, dfOriginal[LABEL_COLUMN], test_size=0.15, random_state=42)\n",
    "input_layer = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "bert_output = bert_model(input_layer)[0]\n",
    "pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(bert_output)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(pooling_layer)\n",
    "\n",
    "modelOriginal = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "modelOriginal.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "modelOriginal.fit(X_trainOriginal, y_trainOriginal, epochs=20, batch_size=32, verbose=2)\n",
    "\n",
    "predictionsOriginal = (modelOriginal.predict(X_testOriginal) > 0.5).astype(int)\n",
    "accuracyOriginal = accuracy_score(y_testOriginal, predictionsOriginal)\n",
    "print(\"Accuracy Overall for Original Dataset:\", accuracyOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587e119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
